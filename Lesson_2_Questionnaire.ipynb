{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff93dfd9-af01-40a3-a652-a0d9569c4570",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "*Welcome to my Fast AI Lesson 2 Questionnaire Notebook!* \n",
    "\n",
    "üöÄ In this notebook, I‚Äôll be diving into the exercises, questions, and reflections from Lesson 2. ü§ìüí° The goal is to practice what I‚Äôve learned, explore deep learning concepts, and have some fun along the way! üñ•Ô∏è‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b04b6-2654-473c-b4cb-f1b1ec92fc0a",
   "metadata": {},
   "source": [
    "## Question 1 ##\n",
    "\n",
    "**Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7024ad2-c166-41a7-a693-fded771eb24f",
   "metadata": {},
   "source": [
    "*Answer*\n",
    "\n",
    "The scenarios where a bear classification model might work poorly in production, due to structual or style differences in the training data include:\n",
    "\n",
    "- The bear might be obstructed by other objects in the image e.g. trees, other animals etc.  The model may fail because it only saw fully visible bears during training.\n",
    "- The bears might in unusual environments e.g. bears caged in a zoo as opposed to out in the wild.\n",
    "- If the training data contained high resolution, production images and the model is passed low-resolution or compressed images it could confuse the model.\n",
    "- The bear training dataset is highly biased towards one type of features (e.g. colour of the bear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e66db-84f8-46fa-aa9c-73b59bb76912",
   "metadata": {},
   "source": [
    "## Question 2 ##\n",
    "\n",
    "**Where do text models currently have a major deficiency?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda4357-0e3b-4a91-b254-d18181fab546",
   "metadata": {},
   "source": [
    "*Answer*\n",
    "\n",
    "Text models have major deficiencies in reliability and understanding beyond patterns in text.  This can manifest in:\n",
    "\n",
    "**1. Factual Accuracy/Hallucinations**\n",
    "\n",
    "A model can confidently generate false or misleading information.  For example, it might invent statistics, quotes or references that don't exist.  Deloitte was caught using AI in $290,000 report to help the Australian government crack down on welfare after a researcher flagged hallucinations.  Sydney University researcher of health and welfare law Chris Rudge said he alerted media outlets that the report was ‚Äúfull of fabricated references.‚Äù\n",
    "\n",
    "**2. Understanding Context or World Knowledge**\n",
    "\n",
    "They don‚Äôt truly ‚Äúknow‚Äù the world; they rely on patterns in text.  Epistemia is a condition where linguistic plausibility replaces real evaluation and this happens alot with LLMs.  Technically, each answer given by an LLM is a probabilistic walk through a giant map of language, not a conclusion reached through reasoning or understanding.\n",
    "\n",
    "Crucially:\n",
    "\n",
    "- The model does not know whether something is true.\n",
    "- It does not understand cause and effect.\n",
    "- It does not realise when it is uncertain.\n",
    "- It cannot decide not to answer.\n",
    "\n",
    "However because LLM answers *sound* plausible, users can be mistaken that the output is relevant to the question being asked.\n",
    "\n",
    "**3. Biases & Ethics**\n",
    "\n",
    "LLMs can inherit the biases present in their training data, reflecting the societal prejudices and imbalances encoded within. Bias in LLMs can lead to discriminatory outputs, perpetuating stereotypes and further marginalizing underrepresented groups.  An example of this is the use of LLMs in the workplace which are supposed to improve efficiency and fairness, but may also reproduce or amplify social biases.\n",
    "\n",
    "The Silicon Ceiling study examined the impact of LLMs on hiring by auditing race and gender bias in OpenAI‚Äôs GPT-3.5.  Researchers conducted two studies using names typically associated with different races and genders:\n",
    "\n",
    "1. Resume evaluation\n",
    "2. Resume generation\n",
    "\n",
    "In Study 1, GPT scores resumes with varied names across multiple occupations and evaluation criteria, revealing stereotype-based biases. In Study 2, GPT generates fictitious resumes, showing systematic differences: women‚Äôs resumes reflect less experience, while Asian and Hispanic resumes include immigrant markers.  These findings are just one example of evidence of bias in LLMs, there are many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c8025-58ac-41a1-bbdc-d0b665fa01a6",
   "metadata": {},
   "source": [
    "## Question 3 ##\n",
    "\n",
    "**What are possible negative societal implications of text generation models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b9a01-ac88-45ad-a0ee-3f502fb2f44e",
   "metadata": {},
   "source": [
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f792e16-42b6-4c68-a5a6-72ac1a40838e",
   "metadata": {},
   "source": [
    "In the paper \"Ethical and social risks of harm from language models\", the authors outline four distinct risks associated with LLMs:\n",
    "\n",
    "1. LLMs can create unfair discrimination and representational and material harm by perpetuating stereotypes and social biases, i.e. harmful associations of specific traits with social identities.\n",
    "2. The risks associated with private data leaks or from LLMs correctly inferring private or other sensitive information. These risks stem from private data that is present in the training corpus and from advanced inference capabilities of LLMs.\n",
    "3. The risk associated with LLMs providing false or misleading information.\n",
    "4. The risks of users or product developers who try to use LMs to cause harm. This includes using LLMs to increase the efficacy of disinformation campaigns, to create personalised scams or fraud at scale, or to develop computer code for viruses or weapon systems.\n",
    "\n",
    "**Reference:** *Weidinger et al., 2023 ‚Äì Ethical and social risks of harm from language models*\n",
    "https://arxiv.org/pdf/2112.04359"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd9326-c41f-45cf-ad87-220e10532121",
   "metadata": {},
   "source": [
    "## Question 4 ##\n",
    "\n",
    "**In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2098b-23ff-43de-984a-c0872f841dd4",
   "metadata": {},
   "source": [
    "Situations where a model might make harmful mistakes are most prevalent in models that make predictions.  An example of this could be a model that predicts disease.  If the model generates a false positive, a patient may be incorrectly informed that they have a disease, causing them unnecessary stress and worry.  Alternatively if the model generates a false negative, a patient, who does in fact have a disease is told that they are disease free, meaning that medical treatment may be delayed.  In both of this scenarios, a model making a mistake could be very harmful.\n",
    "\n",
    "One way of mitigating this risk, is using Human In The Loop, where the model assists humans e.g. by making a prediction on disease status, however a human actively reviews this prediction.  For machine learning use cases in a medical setting, abnormal test results could automatically be flagged for review by a doctor, speeding up diagnostics and reducing risk of patient harm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32c4df-9848-417d-830a-598b359731ac",
   "metadata": {},
   "source": [
    "## Question 5 ##\n",
    "\n",
    "**What kind of tabular data is deep learning particularly good at?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259447f3-0970-4412-ba2d-9bb4aa8bfe59",
   "metadata": {},
   "source": [
    "The tabular data that deep learning is particularly good at, shares these qualitites:\n",
    "\n",
    "**1. Large scale datasets**\n",
    "Datasets containing millions of rows with lots of features contained within the data.\n",
    "\n",
    "**2. Complex Data Interactions**\n",
    "Neural networks can learn non-linear relationships and subtle interactions between features automatically.  For example predicting a users behaviour when there are many categorical and numerical features that interact in complicated ways.  \n",
    "\n",
    "**3. Sequential or time-dependent tabular data**\n",
    "Deep learning (like RNNs, LSTMs, or Transformers) can capture patterns over time.  For example, time series data such as equity prices or sensor readings where data is collected over time.\n",
    "\n",
    "**4. Multi-modal tabular data**\n",
    "Deep learning is effective when tabular features are combined with other modes of data e.g. images or text.  For example, within a medical dataset where patient statistics e.g. blood pressure, heart rate etc are combined with MRI images and doctors notes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd41f88-278d-46f3-ac59-9a220b2557e5",
   "metadata": {},
   "source": [
    "## Question 6 ##\n",
    "\n",
    "**What's a key downside of directly using a deep learning model for recommendation systems?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb82fc3-a00a-4355-8331-46dd9b6ad357",
   "metadata": {},
   "source": [
    "In recommender systems, user/item profiles are established by analysing user engagements with items in the system. User engagement encompasses various actions such as ratings, likes, purchases, views, clicks, comments or any other interaction with items.  The ratings of items is a key data source used by recommender systems to provide personalised recommendations. There are several key downsides of directly using a deep learning model for recommendation systems:\n",
    "\n",
    "**1. Sparsity of User-Item Rating Data**\n",
    "\n",
    "This poses a significant obstacle for Recommender Systems, making it difficult to model user preferences effectively. This issue is particularly evident in collaborative filtering techniques, where the accuracy of user/item similarity calculations and latent factor identification are compromised. Therefore, the sparsity adversely affects the accuracy, coverage, scalability, and transparency of the recommendations.  Deep learning models can have trouble learning meaningful patterns when entries are sparse.\n",
    "\n",
    "**2. User Preferences Instead of Recommendations**\n",
    "\n",
    "Machine learning approaches for recommendation systems will often only tell what products a user might like, and may not be recommendations that would be helpful to the user. For example a user might have previously watched the Lion King (1994).  A machine learning recommender might suggest:\n",
    "\n",
    "- Other versions of the Lion King\n",
    "- The same film again\n",
    "- Films featuring lions\n",
    "\n",
    "While technically correct, these recommendations might not be helpful to the user e.g. the user might prefer animations not specific films about lions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25241fa1-c0e6-47aa-97a1-207110c72b22",
   "metadata": {},
   "source": [
    "## Question 7 ##\n",
    "\n",
    "**What are the steps of the Drivetrain Approach?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5d934-187f-4bad-a5d0-1d92808e58b6",
   "metadata": {},
   "source": [
    "The Drivetrain approach was a method developed in 2012 to ensure that models that are useful are built.  It can be broken down into 4 stages:\n",
    "\n",
    "**1.  Define the Objective  üéØ**\n",
    "- Decide what you are trying to optimise or achieve.\n",
    "- Example: maximize model accuracy, minimize error, or improve user engagement.\n",
    "- This sets the direction for all your efforts.\n",
    "\n",
    "**2. Levers - What Inputs Can We Control?  ‚öôÔ∏è**\n",
    "- Identify the factors you can adjust to influence the objective.\n",
    "- Examples: learning rate, batch size, number of layers, regularisation, or feature selection.\n",
    "- Think of these as the ‚Äúknobs and pedals‚Äù you can tune.\n",
    "\n",
    "**3.  Data - What Data Can We Collect?  üìä**\n",
    "- Determine what information you can gather to help the model.\n",
    "- Examples: more training samples, new features, data augmentation, or cleaned datasets.\n",
    "- Better data often has a bigger impact than tweaking models.\n",
    "\n",
    "**4.  Models - How the Levers Influence the Objective. ü§ñ**\n",
    "- Test how changes in levers and data affect model performance.\n",
    "- Iterate: start simple, scale up, tune hyperparameters, and monitor metrics.\n",
    "- Goal: understand cause-and-effect between inputs and the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e1a2b-1346-4a9b-a30c-db077bba93c9",
   "metadata": {},
   "source": [
    "## Question 8 ##\n",
    "\n",
    "**How do the steps of the Drivetrain Approach map to a recommendation system?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb96f5d-90cd-436d-b8eb-78580adda63c",
   "metadata": {},
   "source": [
    "*Define the Objective*\n",
    "\n",
    "Establish what a 'good' recommendation actually means and how it can be measured.  For example:\n",
    "1. Increase product purchase rate\n",
    "2. Improve user satisfaction or rentention\n",
    "3. Improve user click through rate\n",
    "\n",
    "*Levers - What Inputs Can We Control*\n",
    "\n",
    "What levers can we pull to change or influence the recommendations.  For example\n",
    "1. Ranking strategy (popular vs personalised)\n",
    "2. Exploration vs exploitation balance\n",
    "3. Filters (exclude already-watched or purchased items)\n",
    "4. Frequency and timing of recommendations\n",
    "5. Diversity or novelty constraints\n",
    "\n",
    "*Data - What Data Can We Collect?*\n",
    "\n",
    "A good recommendation system needs behavioural data to help the system learn and improve.  For example:\n",
    "- User interactions (clicks, views, likes, purchases)\n",
    "- Watch time or dwell time\n",
    "- Skips or negative feedback\n",
    "- Context (time of day, device, location)\n",
    "\n",
    "*Models - How the Levers Influence the Objective*\n",
    "\n",
    "The model itself is a tool inside the system.  We can use models to understand how changing the levers affect outcomes:\n",
    "- Predict likelihood a user will engage with an item\n",
    "- Rank items based on predicted usefulness\n",
    "- Test changes using A/B experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0410cdf-b040-4264-bb65-4322bacca258",
   "metadata": {},
   "source": [
    "## Question 9 ##\n",
    "\n",
    "**Create an image recognition model using data you curate, and deploy it on the web.**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9c1bf-7114-4680-9693-06910d118d14",
   "metadata": {},
   "source": [
    "See the Notebook 02_Leaf_Classifier.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a317d37-87e8-4a3a-bae2-f0d113e5b17d",
   "metadata": {},
   "source": [
    "## Question 10 ##\n",
    "\n",
    "**What is DataLoaders?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf0f95a-63e7-4fdf-b439-2399815fb49d",
   "metadata": {},
   "source": [
    "DataLoaders answers the question:\n",
    "\n",
    "*‚ÄúHow do we turn raw data into batches the model can learn from?‚Äù*\n",
    "\n",
    "It‚Äôs one of fastai‚Äôs core ideas: get the data pipeline right first.  Fastai does this via the DataLoaders object which bundles everything together that is needed to feed data into a model, in a clean and reliable way.\n",
    "\n",
    "A DataLoaders object:\n",
    "- Knows where the data comes from\n",
    "- Applies transforms (e.g. resizing images, normalization)\n",
    "- Splits data into training and validation sets\n",
    "- Creates batches for efficient GPU/CPU use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f555819e-2cc5-4f0d-bd15-322efbd11c1a",
   "metadata": {},
   "source": [
    "## Question 11 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777afb8-b726-4eeb-a07e-38652fcfd024",
   "metadata": {},
   "source": [
    "**What four things do we need to tell fastai to create DataLoaders?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d10ae-773f-493d-bb8f-dfc194b9dc59",
   "metadata": {},
   "source": [
    "*1Ô∏è‚É£ What kinds of data we are working with*\n",
    "    \n",
    "    -> Images with categories (ImageBlock, CategoryBlock)\n",
    "\n",
    "data_types = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),  # input type, target type\n",
    "\n",
    "    * 2Ô∏è‚É£ How to get the list of items*\n",
    "    get_items=get_image_files,           # get all image files from a path\n",
    "\n",
    "    * 3Ô∏è‚É£ How to label these items*\n",
    "    get_y=lambda x: x.parent.name,       # extract label from folder name\n",
    "\n",
    "    * 4Ô∏è‚É£ How to create the validation set*\n",
    "    splitter=RandomSplitter(valid_pct=0.2)  # 20% of data for validation\n",
    ")\n",
    "\n",
    "*# Create the DataLoaders*\n",
    "\n",
    "path = Path('data/images')\n",
    "dls = data_types.dataloaders(path, bs=64)  # batch size = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "510c1474-a1c7-42e2-a132-683a3f56d5be",
   "metadata": {},
   "source": [
    "## Question 12 ##\n",
    "\n",
    "**What does the splitter parameter to DataBlock do?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df73b2f-fac6-44e7-b50f-924765c1a583",
   "metadata": {},
   "source": [
    "The splitter parameter in FastAI‚Äôs DataBlock is how you tell FastAI which items go into the training set and which go into the validation set.  FastAI provides several built-in splitters so you usually don‚Äôt have to write your own.\n",
    "\n",
    "| Splitter                                                      | Example                                           | Notes                                                |\n",
    "| ------------------------------------------------------------- | ------------------------------------------------- | ---------------------------------------------------- |\n",
    "| `RandomSplitter(valid_pct=0.2)`                               | 20% random validation                             | Most common                                          |\n",
    "| `GrandparentSplitter(train_name='train', valid_name='valid')` | Uses folder names                                 | Useful for image datasets in `train`/`valid` folders |\n",
    "| `IndexSplitter(valid_idx=some_list)`                          | Custom indices                                    | Gives full control                                   |\n",
    "| `FuncSplitter(func)`                                          | A function that returns True for validation items | Flexible custom logic                                |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74a652db-e2b3-46c8-adb2-0cdfd267ad89",
   "metadata": {},
   "source": [
    "## Question 13 ##\n",
    "\n",
    "**How do we ensure a random split always gives the same validation set?**\n",
    "\n",
    "*Answer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68d47f-d8bc-4cc3-a528-8e8c8a13d673",
   "metadata": {},
   "source": [
    "It is not possible for computers to generate truly random numbers.  To make sure a random split always produces the same validation set, you need to set a random seed before creating the split. This makes the randomness deterministic.  If a seed is not set, every run can produce a different training and validation split, making it harder to compare experiments.  This is because the model could perform differently simply due to different validation data.\n",
    "\n",
    "If your validation set changes every run, performance metrics (accuracy, loss, etc.) can fluctuate just because the data changed, not because your model improved.  Using the same validation set allows you to compare models fairly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1c2bc7b-0ba2-4ff7-b85c-faa471016805",
   "metadata": {},
   "source": [
    "## Question 14 ##\n",
    "\n",
    "**What letters are often used to signify the independent and dependent variables?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523d93a-48e0-445a-8591-17b6a396c152",
   "metadata": {},
   "source": [
    "x is independent. y is dependent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa87511-ead1-40e6-9dc5-f3262d717255",
   "metadata": {},
   "source": [
    "## Question 15 ##\n",
    "\n",
    "**What's the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?**\n",
    "\n",
    "*Answer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2de46-3850-4572-8b69-45d6e590b1c4",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ae9f85a-53a1-47a2-a780-1545ac79667a",
   "metadata": {},
   "source": [
    "## Question 16 ##\n",
    "\n",
    "**What is data augmentation? Why is it needed?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4fa15-24b8-4a57-ab69-cbc883a3cb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1869d96-1c53-457a-a2b5-37ff110dd506",
   "metadata": {},
   "source": [
    "## Question 17 ##\n",
    "\n",
    "**What is the difference between item_tfms and batch_tfms?**\n",
    "\n",
    "*Answer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6295f2-ff34-476a-8211-40c0916a0f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai_env]",
   "language": "python",
   "name": "conda-env-fastai_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
