{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff93dfd9-af01-40a3-a652-a0d9569c4570",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "*Welcome to my Fast AI Lesson 2 Questionnaire Notebook!* \n",
    "\n",
    "üöÄ In this notebook, I‚Äôll be diving into the exercises, questions, and reflections from Lesson 2. ü§ìüí° The goal is to practice what I‚Äôve learned, explore deep learning concepts, and have some fun along the way! üñ•Ô∏è‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b04b6-2654-473c-b4cb-f1b1ec92fc0a",
   "metadata": {},
   "source": [
    "## Question 1 ##\n",
    "\n",
    "**Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7024ad2-c166-41a7-a693-fded771eb24f",
   "metadata": {},
   "source": [
    "*Answer*\n",
    "\n",
    "The scenarios where a bear classification model might work poorly in production, due to structual or style differences in the training data include:\n",
    "\n",
    "- The bear might be obstructed by other objects in the image e.g. trees, other animals etc.  The model may fail because it only saw fully visible bears during training.\n",
    "- The bears might in unusual environments e.g. bears caged in a zoo as opposed to out in the wild.\n",
    "- If the training data contained high resolution, production images and the model is passed low-resolution or compressed images it could confuse the model.\n",
    "- The bear training dataset is highly biased towards one type of features (e.g. colour of the bear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e66db-84f8-46fa-aa9c-73b59bb76912",
   "metadata": {},
   "source": [
    "## Question 2 ##\n",
    "\n",
    "**Where do text models currently have a major deficiency?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda4357-0e3b-4a91-b254-d18181fab546",
   "metadata": {},
   "source": [
    "*Answer*\n",
    "\n",
    "Text models have major deficiencies in reliability and understanding beyond patterns in text.  This can manifest in:\n",
    "\n",
    "**1. Factual Accuracy/Hallucinations**\n",
    "\n",
    "A model can confidently generate false or misleading information.  For example, it might invent statistics, quotes or references that don't exist.  Deloitte was caught using AI in $290,000 report to help the Australian government crack down on welfare after a researcher flagged hallucinations.  Sydney University researcher of health and welfare law Chris Rudge said he alerted media outlets that the report was ‚Äúfull of fabricated references.‚Äù\n",
    "\n",
    "**2. Understanding Context or World Knowledge**\n",
    "\n",
    "They don‚Äôt truly ‚Äúknow‚Äù the world; they rely on patterns in text.  Epistemia is a condition where linguistic plausibility replaces real evaluation and this happens alot with LLMs.  Technically, each answer given by an LLM is a probabilistic walk through a giant map of language, not a conclusion reached through reasoning or understanding.\n",
    "\n",
    "Crucially:\n",
    "\n",
    "- The model does not know whether something is true.\n",
    "- It does not understand cause and effect.\n",
    "- It does not realise when it is uncertain.\n",
    "- It cannot decide not to answer.\n",
    "\n",
    "However because LLM answers *sound* plausible, users can be mistaken that the output is relevant to the question being asked.\n",
    "\n",
    "**3. Biases & Ethics**\n",
    "\n",
    "LLMs can inherit the biases present in their training data, reflecting the societal prejudices and imbalances encoded within. Bias in LLMs can lead to discriminatory outputs, perpetuating stereotypes and further marginalizing underrepresented groups.  An example of this is the use of LLMs in the workplace which are supposed to improve efficiency and fairness, but may also reproduce or amplify social biases.\n",
    "\n",
    "The Silicon Ceiling study examined the impact of LLMs on hiring by auditing race and gender bias in OpenAI‚Äôs GPT-3.5.  Researchers conducted two studies using names typically associated with different races and genders:\n",
    "\n",
    "1. Resume evaluation\n",
    "2. Resume generation\n",
    "\n",
    "In Study 1, GPT scores resumes with varied names across multiple occupations and evaluation criteria, revealing stereotype-based biases. In Study 2, GPT generates fictitious resumes, showing systematic differences: women‚Äôs resumes reflect less experience, while Asian and Hispanic resumes include immigrant markers.  These findings are just one example of evidence of bias in LLMs, there are many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c8025-58ac-41a1-bbdc-d0b665fa01a6",
   "metadata": {},
   "source": [
    "## Question 3 ##\n",
    "\n",
    "**What are possible negative societal implications of text generation models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b9a01-ac88-45ad-a0ee-3f502fb2f44e",
   "metadata": {},
   "source": [
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f792e16-42b6-4c68-a5a6-72ac1a40838e",
   "metadata": {},
   "source": [
    "In the paper \"Ethical and social risks of harm from language models\", the authors outline four distinct risks associated with LLMs:\n",
    "\n",
    "1. LLMs can create unfair discrimination and representational and material harm by perpetuating stereotypes and social biases, i.e. harmful associations of specific traits with social identities.\n",
    "2. The risks associated with private data leaks or from LLMs correctly inferring private or other sensitive information. These risks stem from private data that is present in the training corpus and from advanced inference capabilities of LLMs.\n",
    "3. The risk associated with LLMs providing false or misleading information.\n",
    "4. The risks of users or product developers who try to use LMs to cause harm. This includes using LLMs to increase the efficacy of disinformation campaigns, to create personalised scams or fraud at scale, or to develop computer code for viruses or weapon systems.\n",
    "\n",
    "**Reference:** *Weidinger et al., 2023 ‚Äì Ethical and social risks of harm from language models*\n",
    "https://arxiv.org/pdf/2112.04359"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd9326-c41f-45cf-ad87-220e10532121",
   "metadata": {},
   "source": [
    "## Question 4 ##\n",
    "\n",
    "**In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2098b-23ff-43de-984a-c0872f841dd4",
   "metadata": {},
   "source": [
    "Situations where a model might make harmful mistakes are most prevalent in models that make predictions.  An example of this could be a model that predicts disease.  If the model generates a false positive, a patient may be incorrectly informed that they have a disease, causing them unnecessary stress and worry.  Alternatively if the model generates a false negative, a patient, who does in fact have a disease is told that they are disease free, meaning that medical treatment may be delayed.  In both of this scenarios, a model making a mistake could be very harmful.\n",
    "\n",
    "One way of mitigating this risk, is using Human In The Loop, where the model assists humans e.g. by making a prediction on disease status, however a human actively reviews this prediction.  For machine learning use cases in a medical setting, abnormal test results could automatically be flagged for review by a doctor, speeding up diagnostics and reducing risk of patient harm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32c4df-9848-417d-830a-598b359731ac",
   "metadata": {},
   "source": [
    "## Question 5 ##\n",
    "\n",
    "**What kind of tabular data is deep learning particularly good at?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259447f3-0970-4412-ba2d-9bb4aa8bfe59",
   "metadata": {},
   "source": [
    "The tabular data that deep learning is particularly good at, shares these qualitites:\n",
    "\n",
    "**1. Large scale datasets**\n",
    "Datasets containing millions of rows with lots of features contained within the data.\n",
    "\n",
    "**2. Complex Data Interactions**\n",
    "Neural networks can learn non-linear relationships and subtle interactions between features automatically.  For example predicting a users behaviour when there are many categorical and numerical features that interact in complicated ways.  \n",
    "\n",
    "**3. Sequential or time-dependent tabular data**\n",
    "Deep learning (like RNNs, LSTMs, or Transformers) can capture patterns over time.  For example, time series data such as equity prices or sensor readings where data is collected over time.\n",
    "\n",
    "**4. Multi-modal tabular data**\n",
    "Deep learning is effective when tabular features are combined with other modes of data e.g. images or text.  For example, within a medical dataset where patient statistics e.g. blood pressure, heart rate etc are combined with MRI images and doctors notes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd41f88-278d-46f3-ac59-9a220b2557e5",
   "metadata": {},
   "source": [
    "## Question 6 ##\n",
    "\n",
    "**What's a key downside of directly using a deep learning model for recommendation systems?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb82fc3-a00a-4355-8331-46dd9b6ad357",
   "metadata": {},
   "source": [
    "In recommender systems, user/item profiles are established by analysing user engagements with items in the system. User engagement encompasses various actions such as ratings, likes, purchases, views, clicks, comments or any other interaction with items.  The ratings of items is a key data source used by recommender systems to provide personalised recommendations. There are several key downsides of directly using a deep learning model for recommendation systems:\n",
    "\n",
    "**1. Sparsity of User-Item Rating Data**\n",
    "\n",
    "This poses a significant obstacle for Recommender Systems, making it difficult to model user preferences effectively. This issue is particularly evident in collaborative filtering techniques, where the accuracy of user/item similarity calculations and latent factor identification are compromised. Therefore, the sparsity adversely affects the accuracy, coverage, scalability, and transparency of the recommendations.  Deep learning models can have trouble learning meaningful patterns when entries are sparse.\n",
    "\n",
    "**2. User Preferences Instead of Recommendations**\n",
    "\n",
    "Machine learning approaches for recommendation systems will often only tell what products a user might like, and may not be recommendations that would be helpful to the user. For example a user might have previously watched the Lion King (1994).  A machine learning recommender might suggest:\n",
    "\n",
    "- Other versions of the Lion King\n",
    "- The same film again\n",
    "- Films featuring lions\n",
    "\n",
    "While technically correct, these recommendations might not be helpful to the user e.g. the user might prefer animations not specific films about lions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25241fa1-c0e6-47aa-97a1-207110c72b22",
   "metadata": {},
   "source": [
    "## Question 7 ##\n",
    "\n",
    "**What are the steps of the Drivetrain Approach?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5d934-187f-4bad-a5d0-1d92808e58b6",
   "metadata": {},
   "source": [
    "The Drivetrain approach was a method developed in 2012 to ensure that models that are useful are built.  It can be broken down into 4 stages:\n",
    "\n",
    "**1.  Define the Objective  üéØ**\n",
    "- Decide what you are trying to optimise or achieve.\n",
    "- Example: maximize model accuracy, minimize error, or improve user engagement.\n",
    "- This sets the direction for all your efforts.\n",
    "\n",
    "**2. Levers - What Inputs Can We Control?  ‚öôÔ∏è**\n",
    "- Identify the factors you can adjust to influence the objective.\n",
    "- Examples: learning rate, batch size, number of layers, regularisation, or feature selection.\n",
    "- Think of these as the ‚Äúknobs and pedals‚Äù you can tune.\n",
    "\n",
    "**3.  Data - What Data Can We Collect?  üìä**\n",
    "- Determine what information you can gather to help the model.\n",
    "- Examples: more training samples, new features, data augmentation, or cleaned datasets.\n",
    "- Better data often has a bigger impact than tweaking models.\n",
    "\n",
    "**4.  Models - How the Levers Influence the Objective. ü§ñ**\n",
    "- Test how changes in levers and data affect model performance.\n",
    "- Iterate: start simple, scale up, tune hyperparameters, and monitor metrics.\n",
    "- Goal: understand cause-and-effect between inputs and the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e1a2b-1346-4a9b-a30c-db077bba93c9",
   "metadata": {},
   "source": [
    "## Question 8 ##\n",
    "\n",
    "**How do the steps of the Drivetrain Approach map to a recommendation system?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f3213-2a46-4d0e-b474-49688b448082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0410cdf-b040-4264-bb65-4322bacca258",
   "metadata": {},
   "source": [
    "## Question 9 ##\n",
    "\n",
    "**Create an image recognition model using data you curate, and deploy it on the web.**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca44b73-85e6-416f-bd7d-88da54303d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a317d37-87e8-4a3a-bae2-f0d113e5b17d",
   "metadata": {},
   "source": [
    "## Question 10 ##\n",
    "\n",
    "**What is DataLoaders?**\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2cf901-fd31-401c-93e1-09ad139b6434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai_env]",
   "language": "python",
   "name": "conda-env-fastai_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
