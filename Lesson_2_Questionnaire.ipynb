{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff93dfd9-af01-40a3-a652-a0d9569c4570",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "*Welcome to my Fast AI Lesson 2 Questionnaire Notebook!* \n",
    "\n",
    "üöÄ In this notebook, I‚Äôll be diving into the exercises, questions, and reflections from Lesson 2. ü§ìüí° The goal is to practice what I‚Äôve learned, explore deep learning concepts, and have some fun along the way! üñ•Ô∏è‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b04b6-2654-473c-b4cb-f1b1ec92fc0a",
   "metadata": {},
   "source": [
    "## Question 1 ##\n",
    "\n",
    "**Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7024ad2-c166-41a7-a693-fded771eb24f",
   "metadata": {},
   "source": [
    "*Answer*\n",
    "\n",
    "The scenarios where a bear classification model might work poorly in production, due to structual or style differences in the training data include:\n",
    "\n",
    "- The bear might be obstructed by other objects in the image e.g. trees, other animals etc.  The model may fail because it only saw fully visible bears during training.\n",
    "- The bears might in unusual environments e.g. bears caged in a zoo as opposed to out in the wild.\n",
    "- If the training data contained high resolution, production images and the model is passed low-resolution or compressed images it could confuse the model.\n",
    "- The bear training dataset is highly biased towards one type of features (e.g. colour of the bear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e66db-84f8-46fa-aa9c-73b59bb76912",
   "metadata": {},
   "source": [
    "## Question 2 ##\n",
    "\n",
    "**Where do text models currently have a major deficiency?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda4357-0e3b-4a91-b254-d18181fab546",
   "metadata": {},
   "source": [
    "*Answer*\n",
    "\n",
    "Text models have major deficiencies in reliability and understanding beyond patterns in text.  This can manifest in:\n",
    "\n",
    "**1. Factual Accuracy/Hallucinations**\n",
    "\n",
    "A model can confidently generate false or misleading information.  For example, it might invent statistics, quotes or references that don't exist.  Deloitte was caught using AI in $290,000 report to help the Australian government crack down on welfare after a researcher flagged hallucinations.  Sydney University researcher of health and welfare law Chris Rudge said he alerted media outlets that the report was ‚Äúfull of fabricated references.‚Äù\n",
    "\n",
    "**2. Understanding Context or World Knowledge**\n",
    "\n",
    "They don‚Äôt truly ‚Äúknow‚Äù the world; they rely on patterns in text.  Epistemia is a condition where linguistic plausibility replaces real evaluation and this happens alot with LLMs.  Technically, each answer given by an LLM is a probabilistic walk through a giant map of language, not a conclusion reached through reasoning or understanding.\n",
    "\n",
    "Crucially:\n",
    "\n",
    "- The model does not know whether something is true.\n",
    "- It does not understand cause and effect.\n",
    "- It does not realise when it is uncertain.\n",
    "- It cannot decide not to answer.\n",
    "\n",
    "However because LLM answers *sound* plausible, users can be mistaken that the output is relevant to the question being asked.\n",
    "\n",
    "**3. Biases & Ethics**\n",
    "\n",
    "LLMs can inherit the biases present in their training data, reflecting the societal prejudices and imbalances encoded within. Bias in LLMs can lead to discriminatory outputs, perpetuating stereotypes and further marginalizing underrepresented groups.  An example of this is the use of LLMs in the workplace which are supposed to improve efficiency and fairness, but may also reproduce or amplify social biases.\n",
    "\n",
    "The Silicon Ceiling study examined the impact of LLMs on hiring by auditing race and gender bias in OpenAI‚Äôs GPT-3.5.  Researchers conducted two studies using names typically associated with different races and genders:\n",
    "\n",
    "1. Resume evaluation\n",
    "2. Resume generation\n",
    "\n",
    "In Study 1, GPT scores resumes with varied names across multiple occupations and evaluation criteria, revealing stereotype-based biases. In Study 2, GPT generates fictitious resumes, showing systematic differences: women‚Äôs resumes reflect less experience, while Asian and Hispanic resumes include immigrant markers.  These findings are just one example of evidence of bias in LLMs, there are many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c8025-58ac-41a1-bbdc-d0b665fa01a6",
   "metadata": {},
   "source": [
    "## Question 3 ##\n",
    "\n",
    "**What are possible negative societal implications of text generation models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b9a01-ac88-45ad-a0ee-3f502fb2f44e",
   "metadata": {},
   "source": [
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709a6cc-fb63-4aec-8b7e-db90103c0b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai_env]",
   "language": "python",
   "name": "conda-env-fastai_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
