{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435ef012-1840-4014-9c27-c754c892808f",
   "metadata": {},
   "source": [
    "# MNIST Basics: Neural Networks from Scratch #\n",
    "\n",
    "In this notebook, we will explore the fundamentals of neural networks by working with the MNIST dataset of handwritten digits. We'll start by visualising the data, then build a simple classifier from scratch — no high-level models yet! Along the way, we’ll learn how to represent images as tensors, compute distances, and gradually move toward a basic neural network to recognize 3s and 7s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0d47b4-1a98-4375-a024-d810d0275586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fastai_env/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/fastai_env/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /opt/anaconda3/envs/fastai_env/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <C62FA868-4C49-3B20-AFA6-2F7F59A295B4> /opt/anaconda3/envs/fastai_env/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521bf7e0-9a43-4558-9142-e40a3f095ccb",
   "metadata": {},
   "source": [
    "For this initial tutorial we are just going to try to create a model that can classify any image as a 3 or a 7. \n",
    "\n",
    "### Download Sample of MNIST ###\n",
    "\n",
    "So let's download a sample of MNIST that contains images of just these digits:\n",
    "\n",
    "- URLs is a FastAI class that contains links to commonly used datasets.\n",
    "- untar_data( ) is a FastAI utility function that:\n",
    "    1. Downloads the dataset from the URL\n",
    "    2. Extracts it (hence 'untar') and returns a Path object pointing to the local folder where the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d4bf1a-75cb-4dd0-8dec-0ebfd1df793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f43f3-5680-4775-8860-920b404c105e",
   "metadata": {},
   "source": [
    "Set the default folder for all file paths in this notebook to be the MNIST sample dataset folder, so I don’t have to type the full path every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbaea525-1426-4d50-a416-b03447a777e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# Set the default base folder for all Path objects in this notebook\n",
    "# This means that later, when we refer to files with relative paths,\n",
    "# FastAI will look inside the MNIST_SAMPLE folder automatically\n",
    "\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c00f84-ae99-4b14-bd11-126e333c4bf3",
   "metadata": {},
   "source": [
    "Next we want to take a look in the folder.\n",
    "\n",
    "ls( ) lists everything in this folder.  However with FastAI, instead of returning a list, it returns an L object.  It acts as a normal list that can be looped, indexed over but also, when you print it, it tells you how many things are in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98308ff3-dae3-45ad-a741-d292c56a20e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('valid'),Path('labels.csv'),Path('train')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786213a3-8ed9-4b8c-bafb-b562f0f1cd0b",
   "metadata": {},
   "source": [
    "The MNIST dataset follows a common layout for machine learning datasets: separate folders for the training set and the validation set (and/or test set). Let's see what's inside the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39cb498-747b-4534-bc17-87c0358c3d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('train/7'),Path('train/3')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f82642-61db-445a-a007-4929006af091",
   "metadata": {},
   "source": [
    "There's a folder of 3s, and a folder of 7s. In machine learning parlance, we say that \"3\" and \"7\" are the labels (or targets) in this dataset. \n",
    "\n",
    "Let's take a look in one of these folders (using sorted to ensure we all get the same order of files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24aef3df-6bdd-43e3-bcb9-39004c2db5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png'),Path('train/3/10093.png'),Path('train/3/10097.png'),Path('train/3/10099.png'),Path('train/3/10116.png'),Path('train/3/10125.png'),Path('train/3/10137.png'),Path('train/3/10141.png'),Path('train/3/10144.png'),Path('train/3/10155.png'),Path('train/3/10161.png')...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threes = (path/'train'/'3').ls().sorted()\n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "threes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e490f-7ed2-4312-9602-b1313024ffda",
   "metadata": {},
   "source": [
    "As we might expect, it's full of image files. Let’s take a look at one now. Here’s an image of a handwritten number 3, taken from the famous MNIST dataset of handwritten numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba91338e-80c9-4851-8d50-fa07d3a7ab0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APCdP02+1a8W006znu7l+VigjLsQOpwO1dlb/CjVnkS3vNb8O6dqDkKtheaiFn3Hou1QcE8YBPcVg+LfCGqeCtYXS9WEPnvEJlML7lKEkA547qawatafqd/pN2LvTb24s7lQQJbeQowB6jI5r1D4Y6b4OvdXtdf8S+K45NY85phY3W6MeaCdrSTNw3IDcd8ZzyKx/i5pXiiLxMNZ8RC1kjvx/os1nJvh2KOFU8HgEHkc5zXntWdP0681a/hsbC3kubqY7Y4oxlmPXiu68OfBzxPq1yJNVtW0XTI/mnu73CbF74UkEn64HvVn4r+LdI1GDR/C3h2QzaTosXli5JJ858BeM9QAOvck9sV5nU1rdXFjdR3VpPLb3ETbo5YnKOh9QRyDV7UfE2v6vbi31PXNSvYA24RXN3JIufXDEjNZdFf/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9UlEQVR4AWNgGGSAEe4e2Upda8b/mwSvnloAF4MyDJY9+Pv31ZG/QPASTS72y8+/u/W4OFi4DiFLsoCV8XEyvCy9BGT++cfAsBlNJ7OICD9YSPvB369eaJJw7s+/X+vgHBQGX+r9vz9qUIRgHO5l74FO3S0H46PQAj9+gTzy4mo+E4o4lKOdlHQBJL9bDJssAwOncTlQ1ge7JAMD4/a/f7uhkhjm///PwHAXl87Qn3//quCQtLn29+9abuySSd///n3EiVVOa9ofYKSYostpFWvZFM//8Pfvr/WS6HIMu0GeB4KjYRhSDAzpYKkXzohUg0URtYQA/HZrR+ekLi0AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the second image file path from the list of '3' images\n",
    "im3_path = threes[1]\n",
    "\n",
    "# Open the image at that path using PIL (Python Imaging Library)\n",
    "im3 = Image.open(im3_path)\n",
    "\n",
    "# Display the image in the notebook\n",
    "im3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcb661-c05a-4b5b-ae6a-bc8448c4462a",
   "metadata": {},
   "source": [
    "In a computer, everything is represented as a number. To view the numbers that make up this image, we have to convert it to a NumPy array or a PyTorch tensor. For instance, here's what a section of the image looks like, converted to a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485bccde-8a1e-4f54-af6f-613aff8f74bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  29],\n",
       "       [  0,   0,   0,  48, 166, 224],\n",
       "       [  0,  93, 244, 249, 253, 187],\n",
       "       [  0, 107, 253, 253, 230,  48],\n",
       "       [  0,   3,  20,  20,  15,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the image `im3` to a NumPy array and extract a small 6x6 patch\n",
    "# from rows 4 to 9 and columns 4 to 9 (Python slicing excludes the end index)\n",
    "array(im3)[4:10, 4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a3c57-9ccd-4fd1-a1c9-8141309693de",
   "metadata": {},
   "source": [
    "Here's the same thing as a PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b8b362-fb51-45a1-adb3-db4cd41e25ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  29],\n",
       "        [  0,   0,   0,  48, 166, 224],\n",
       "        [  0,  93, 244, 249, 253, 187],\n",
       "        [  0, 107, 253, 253, 230,  48],\n",
       "        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the image `im3` to a PyTorch tensor and extract a small 6x6 patch\n",
    "# from rows 4 to 9 and columns 4 to 9 (Python slicing excludes the end index)\n",
    "tensor(im3)[4:10, 4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29dd15-47c9-46a3-8c41-488d7fe6d9c5",
   "metadata": {},
   "source": [
    "We can slice the array to pick just the part with the top of the digit in it, and then use a Pandas DataFrame to color-code the values using a gradient, which shows us clearly how the image is created from the pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696af7f7-e127-4bd8-a49f-ba01ac66fa7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_40a0c_row0_col0, #T_40a0c_row0_col1, #T_40a0c_row0_col2, #T_40a0c_row0_col3, #T_40a0c_row0_col4, #T_40a0c_row0_col5, #T_40a0c_row0_col6, #T_40a0c_row0_col7, #T_40a0c_row0_col8, #T_40a0c_row0_col9, #T_40a0c_row0_col10, #T_40a0c_row0_col11, #T_40a0c_row0_col12, #T_40a0c_row0_col13, #T_40a0c_row0_col14, #T_40a0c_row0_col15, #T_40a0c_row0_col16, #T_40a0c_row0_col17, #T_40a0c_row1_col0, #T_40a0c_row1_col1, #T_40a0c_row1_col2, #T_40a0c_row1_col3, #T_40a0c_row1_col4, #T_40a0c_row1_col15, #T_40a0c_row1_col16, #T_40a0c_row1_col17, #T_40a0c_row2_col0, #T_40a0c_row2_col1, #T_40a0c_row2_col2, #T_40a0c_row2_col15, #T_40a0c_row2_col16, #T_40a0c_row2_col17, #T_40a0c_row3_col0, #T_40a0c_row3_col15, #T_40a0c_row3_col16, #T_40a0c_row3_col17, #T_40a0c_row4_col0, #T_40a0c_row4_col6, #T_40a0c_row4_col7, #T_40a0c_row4_col8, #T_40a0c_row4_col9, #T_40a0c_row4_col10, #T_40a0c_row4_col15, #T_40a0c_row4_col16, #T_40a0c_row4_col17, #T_40a0c_row5_col0, #T_40a0c_row5_col5, #T_40a0c_row5_col6, #T_40a0c_row5_col7, #T_40a0c_row5_col8, #T_40a0c_row5_col9, #T_40a0c_row5_col15, #T_40a0c_row5_col16, #T_40a0c_row5_col17, #T_40a0c_row6_col0, #T_40a0c_row6_col1, #T_40a0c_row6_col2, #T_40a0c_row6_col3, #T_40a0c_row6_col4, #T_40a0c_row6_col5, #T_40a0c_row6_col6, #T_40a0c_row6_col7, #T_40a0c_row6_col8, #T_40a0c_row6_col9, #T_40a0c_row6_col14, #T_40a0c_row6_col15, #T_40a0c_row6_col16, #T_40a0c_row6_col17, #T_40a0c_row7_col0, #T_40a0c_row7_col1, #T_40a0c_row7_col2, #T_40a0c_row7_col3, #T_40a0c_row7_col4, #T_40a0c_row7_col5, #T_40a0c_row7_col6, #T_40a0c_row7_col13, #T_40a0c_row7_col14, #T_40a0c_row7_col15, #T_40a0c_row7_col16, #T_40a0c_row7_col17, #T_40a0c_row8_col0, #T_40a0c_row8_col1, #T_40a0c_row8_col2, #T_40a0c_row8_col3, #T_40a0c_row8_col4, #T_40a0c_row8_col13, #T_40a0c_row8_col14, #T_40a0c_row8_col15, #T_40a0c_row8_col16, #T_40a0c_row8_col17, #T_40a0c_row9_col0, #T_40a0c_row9_col1, #T_40a0c_row9_col2, #T_40a0c_row9_col3, #T_40a0c_row9_col4, #T_40a0c_row9_col16, #T_40a0c_row9_col17, #T_40a0c_row10_col0, #T_40a0c_row10_col1, #T_40a0c_row10_col2, #T_40a0c_row10_col3, #T_40a0c_row10_col4, #T_40a0c_row10_col5, #T_40a0c_row10_col6, #T_40a0c_row10_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #ffffff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row1_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #efefef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row1_col6, #T_40a0c_row1_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #7c7c7c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row1_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4a4a4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row1_col8, #T_40a0c_row1_col9, #T_40a0c_row1_col10, #T_40a0c_row2_col5, #T_40a0c_row2_col6, #T_40a0c_row2_col7, #T_40a0c_row2_col11, #T_40a0c_row2_col12, #T_40a0c_row2_col13, #T_40a0c_row3_col4, #T_40a0c_row3_col12, #T_40a0c_row3_col13, #T_40a0c_row4_col1, #T_40a0c_row4_col2, #T_40a0c_row4_col3, #T_40a0c_row4_col12, #T_40a0c_row4_col13, #T_40a0c_row5_col12, #T_40a0c_row6_col11, #T_40a0c_row9_col11, #T_40a0c_row10_col11, #T_40a0c_row10_col12, #T_40a0c_row10_col13, #T_40a0c_row10_col14, #T_40a0c_row10_col15, #T_40a0c_row10_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row1_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #606060;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row1_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4d4d4d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row1_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bbbbbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row2_col3 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e4e4e4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row2_col4, #T_40a0c_row8_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #6b6b6b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row2_col8, #T_40a0c_row2_col14, #T_40a0c_row3_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #171717;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row2_col9, #T_40a0c_row3_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4b4b4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row2_col10, #T_40a0c_row7_col10, #T_40a0c_row8_col8, #T_40a0c_row8_col10, #T_40a0c_row9_col8, #T_40a0c_row9_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #010101;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row3_col1 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #272727;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row3_col2 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #0a0a0a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row3_col3 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #050505;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row3_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #333333;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row3_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e6e6e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row3_col7, #T_40a0c_row3_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fafafa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row3_col8 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fbfbfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row3_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fdfdfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row4_col4 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #1b1b1b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row4_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e0e0e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row4_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4e4e4e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row4_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #767676;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row5_col1 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fcfcfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row5_col2, #T_40a0c_row5_col3 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f6f6f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row5_col4, #T_40a0c_row7_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f8f8f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row5_col10, #T_40a0c_row10_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e8e8e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row5_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #222222;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row5_col13, #T_40a0c_row6_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #090909;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row5_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d0d0d0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row6_col10, #T_40a0c_row7_col11, #T_40a0c_row9_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #060606;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row6_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #979797;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row7_col8 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #b6b6b6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row7_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #252525;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row7_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #999999;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row8_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f9f9f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row8_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #101010;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row8_col9, #T_40a0c_row9_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #020202;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row8_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #545454;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row8_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f1f1f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row9_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f7f7f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row9_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #030303;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row9_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #181818;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row9_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #303030;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row9_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #a9a9a9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_40a0c_row9_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fefefe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row10_col8, #T_40a0c_row10_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bababa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_40a0c_row10_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #393939;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_40a0c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_40a0c_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_40a0c_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_40a0c_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_40a0c_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_40a0c_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_40a0c_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
       "      <th id=\"T_40a0c_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
       "      <th id=\"T_40a0c_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n",
       "      <th id=\"T_40a0c_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n",
       "      <th id=\"T_40a0c_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n",
       "      <th id=\"T_40a0c_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n",
       "      <th id=\"T_40a0c_level0_col11\" class=\"col_heading level0 col11\" >11</th>\n",
       "      <th id=\"T_40a0c_level0_col12\" class=\"col_heading level0 col12\" >12</th>\n",
       "      <th id=\"T_40a0c_level0_col13\" class=\"col_heading level0 col13\" >13</th>\n",
       "      <th id=\"T_40a0c_level0_col14\" class=\"col_heading level0 col14\" >14</th>\n",
       "      <th id=\"T_40a0c_level0_col15\" class=\"col_heading level0 col15\" >15</th>\n",
       "      <th id=\"T_40a0c_level0_col16\" class=\"col_heading level0 col16\" >16</th>\n",
       "      <th id=\"T_40a0c_level0_col17\" class=\"col_heading level0 col17\" >17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_40a0c_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col8\" class=\"data row0 col8\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col9\" class=\"data row0 col9\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col10\" class=\"data row0 col10\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col12\" class=\"data row0 col12\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col13\" class=\"data row0 col13\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col14\" class=\"data row0 col14\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col15\" class=\"data row0 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col16\" class=\"data row0 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row0_col17\" class=\"data row0 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_40a0c_row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_40a0c_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_40a0c_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_40a0c_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_40a0c_row1_col5\" class=\"data row1 col5\" >29</td>\n",
       "      <td id=\"T_40a0c_row1_col6\" class=\"data row1 col6\" >150</td>\n",
       "      <td id=\"T_40a0c_row1_col7\" class=\"data row1 col7\" >195</td>\n",
       "      <td id=\"T_40a0c_row1_col8\" class=\"data row1 col8\" >254</td>\n",
       "      <td id=\"T_40a0c_row1_col9\" class=\"data row1 col9\" >255</td>\n",
       "      <td id=\"T_40a0c_row1_col10\" class=\"data row1 col10\" >254</td>\n",
       "      <td id=\"T_40a0c_row1_col11\" class=\"data row1 col11\" >176</td>\n",
       "      <td id=\"T_40a0c_row1_col12\" class=\"data row1 col12\" >193</td>\n",
       "      <td id=\"T_40a0c_row1_col13\" class=\"data row1 col13\" >150</td>\n",
       "      <td id=\"T_40a0c_row1_col14\" class=\"data row1 col14\" >96</td>\n",
       "      <td id=\"T_40a0c_row1_col15\" class=\"data row1 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row1_col16\" class=\"data row1 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row1_col17\" class=\"data row1 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_40a0c_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_40a0c_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_40a0c_row2_col3\" class=\"data row2 col3\" >48</td>\n",
       "      <td id=\"T_40a0c_row2_col4\" class=\"data row2 col4\" >166</td>\n",
       "      <td id=\"T_40a0c_row2_col5\" class=\"data row2 col5\" >224</td>\n",
       "      <td id=\"T_40a0c_row2_col6\" class=\"data row2 col6\" >253</td>\n",
       "      <td id=\"T_40a0c_row2_col7\" class=\"data row2 col7\" >253</td>\n",
       "      <td id=\"T_40a0c_row2_col8\" class=\"data row2 col8\" >234</td>\n",
       "      <td id=\"T_40a0c_row2_col9\" class=\"data row2 col9\" >196</td>\n",
       "      <td id=\"T_40a0c_row2_col10\" class=\"data row2 col10\" >253</td>\n",
       "      <td id=\"T_40a0c_row2_col11\" class=\"data row2 col11\" >253</td>\n",
       "      <td id=\"T_40a0c_row2_col12\" class=\"data row2 col12\" >253</td>\n",
       "      <td id=\"T_40a0c_row2_col13\" class=\"data row2 col13\" >253</td>\n",
       "      <td id=\"T_40a0c_row2_col14\" class=\"data row2 col14\" >233</td>\n",
       "      <td id=\"T_40a0c_row2_col15\" class=\"data row2 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row2_col16\" class=\"data row2 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row2_col17\" class=\"data row2 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_40a0c_row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row3_col1\" class=\"data row3 col1\" >93</td>\n",
       "      <td id=\"T_40a0c_row3_col2\" class=\"data row3 col2\" >244</td>\n",
       "      <td id=\"T_40a0c_row3_col3\" class=\"data row3 col3\" >249</td>\n",
       "      <td id=\"T_40a0c_row3_col4\" class=\"data row3 col4\" >253</td>\n",
       "      <td id=\"T_40a0c_row3_col5\" class=\"data row3 col5\" >187</td>\n",
       "      <td id=\"T_40a0c_row3_col6\" class=\"data row3 col6\" >46</td>\n",
       "      <td id=\"T_40a0c_row3_col7\" class=\"data row3 col7\" >10</td>\n",
       "      <td id=\"T_40a0c_row3_col8\" class=\"data row3 col8\" >8</td>\n",
       "      <td id=\"T_40a0c_row3_col9\" class=\"data row3 col9\" >4</td>\n",
       "      <td id=\"T_40a0c_row3_col10\" class=\"data row3 col10\" >10</td>\n",
       "      <td id=\"T_40a0c_row3_col11\" class=\"data row3 col11\" >194</td>\n",
       "      <td id=\"T_40a0c_row3_col12\" class=\"data row3 col12\" >253</td>\n",
       "      <td id=\"T_40a0c_row3_col13\" class=\"data row3 col13\" >253</td>\n",
       "      <td id=\"T_40a0c_row3_col14\" class=\"data row3 col14\" >233</td>\n",
       "      <td id=\"T_40a0c_row3_col15\" class=\"data row3 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row3_col16\" class=\"data row3 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row3_col17\" class=\"data row3 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_40a0c_row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row4_col1\" class=\"data row4 col1\" >107</td>\n",
       "      <td id=\"T_40a0c_row4_col2\" class=\"data row4 col2\" >253</td>\n",
       "      <td id=\"T_40a0c_row4_col3\" class=\"data row4 col3\" >253</td>\n",
       "      <td id=\"T_40a0c_row4_col4\" class=\"data row4 col4\" >230</td>\n",
       "      <td id=\"T_40a0c_row4_col5\" class=\"data row4 col5\" >48</td>\n",
       "      <td id=\"T_40a0c_row4_col6\" class=\"data row4 col6\" >0</td>\n",
       "      <td id=\"T_40a0c_row4_col7\" class=\"data row4 col7\" >0</td>\n",
       "      <td id=\"T_40a0c_row4_col8\" class=\"data row4 col8\" >0</td>\n",
       "      <td id=\"T_40a0c_row4_col9\" class=\"data row4 col9\" >0</td>\n",
       "      <td id=\"T_40a0c_row4_col10\" class=\"data row4 col10\" >0</td>\n",
       "      <td id=\"T_40a0c_row4_col11\" class=\"data row4 col11\" >192</td>\n",
       "      <td id=\"T_40a0c_row4_col12\" class=\"data row4 col12\" >253</td>\n",
       "      <td id=\"T_40a0c_row4_col13\" class=\"data row4 col13\" >253</td>\n",
       "      <td id=\"T_40a0c_row4_col14\" class=\"data row4 col14\" >156</td>\n",
       "      <td id=\"T_40a0c_row4_col15\" class=\"data row4 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row4_col16\" class=\"data row4 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row4_col17\" class=\"data row4 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_40a0c_row5_col0\" class=\"data row5 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row5_col1\" class=\"data row5 col1\" >3</td>\n",
       "      <td id=\"T_40a0c_row5_col2\" class=\"data row5 col2\" >20</td>\n",
       "      <td id=\"T_40a0c_row5_col3\" class=\"data row5 col3\" >20</td>\n",
       "      <td id=\"T_40a0c_row5_col4\" class=\"data row5 col4\" >15</td>\n",
       "      <td id=\"T_40a0c_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_40a0c_row5_col6\" class=\"data row5 col6\" >0</td>\n",
       "      <td id=\"T_40a0c_row5_col7\" class=\"data row5 col7\" >0</td>\n",
       "      <td id=\"T_40a0c_row5_col8\" class=\"data row5 col8\" >0</td>\n",
       "      <td id=\"T_40a0c_row5_col9\" class=\"data row5 col9\" >0</td>\n",
       "      <td id=\"T_40a0c_row5_col10\" class=\"data row5 col10\" >43</td>\n",
       "      <td id=\"T_40a0c_row5_col11\" class=\"data row5 col11\" >224</td>\n",
       "      <td id=\"T_40a0c_row5_col12\" class=\"data row5 col12\" >253</td>\n",
       "      <td id=\"T_40a0c_row5_col13\" class=\"data row5 col13\" >245</td>\n",
       "      <td id=\"T_40a0c_row5_col14\" class=\"data row5 col14\" >74</td>\n",
       "      <td id=\"T_40a0c_row5_col15\" class=\"data row5 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row5_col16\" class=\"data row5 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row5_col17\" class=\"data row5 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_40a0c_row6_col0\" class=\"data row6 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col3\" class=\"data row6 col3\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col4\" class=\"data row6 col4\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col5\" class=\"data row6 col5\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col6\" class=\"data row6 col6\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col7\" class=\"data row6 col7\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col8\" class=\"data row6 col8\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col9\" class=\"data row6 col9\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col10\" class=\"data row6 col10\" >249</td>\n",
       "      <td id=\"T_40a0c_row6_col11\" class=\"data row6 col11\" >253</td>\n",
       "      <td id=\"T_40a0c_row6_col12\" class=\"data row6 col12\" >245</td>\n",
       "      <td id=\"T_40a0c_row6_col13\" class=\"data row6 col13\" >126</td>\n",
       "      <td id=\"T_40a0c_row6_col14\" class=\"data row6 col14\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col15\" class=\"data row6 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col16\" class=\"data row6 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row6_col17\" class=\"data row6 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_40a0c_row7_col0\" class=\"data row7 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col4\" class=\"data row7 col4\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col5\" class=\"data row7 col5\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col6\" class=\"data row7 col6\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col7\" class=\"data row7 col7\" >14</td>\n",
       "      <td id=\"T_40a0c_row7_col8\" class=\"data row7 col8\" >101</td>\n",
       "      <td id=\"T_40a0c_row7_col9\" class=\"data row7 col9\" >223</td>\n",
       "      <td id=\"T_40a0c_row7_col10\" class=\"data row7 col10\" >253</td>\n",
       "      <td id=\"T_40a0c_row7_col11\" class=\"data row7 col11\" >248</td>\n",
       "      <td id=\"T_40a0c_row7_col12\" class=\"data row7 col12\" >124</td>\n",
       "      <td id=\"T_40a0c_row7_col13\" class=\"data row7 col13\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col14\" class=\"data row7 col14\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col15\" class=\"data row7 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col16\" class=\"data row7 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row7_col17\" class=\"data row7 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_40a0c_row8_col0\" class=\"data row8 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col3\" class=\"data row8 col3\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col4\" class=\"data row8 col4\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col5\" class=\"data row8 col5\" >11</td>\n",
       "      <td id=\"T_40a0c_row8_col6\" class=\"data row8 col6\" >166</td>\n",
       "      <td id=\"T_40a0c_row8_col7\" class=\"data row8 col7\" >239</td>\n",
       "      <td id=\"T_40a0c_row8_col8\" class=\"data row8 col8\" >253</td>\n",
       "      <td id=\"T_40a0c_row8_col9\" class=\"data row8 col9\" >253</td>\n",
       "      <td id=\"T_40a0c_row8_col10\" class=\"data row8 col10\" >253</td>\n",
       "      <td id=\"T_40a0c_row8_col11\" class=\"data row8 col11\" >187</td>\n",
       "      <td id=\"T_40a0c_row8_col12\" class=\"data row8 col12\" >30</td>\n",
       "      <td id=\"T_40a0c_row8_col13\" class=\"data row8 col13\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col14\" class=\"data row8 col14\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col15\" class=\"data row8 col15\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col16\" class=\"data row8 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row8_col17\" class=\"data row8 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_40a0c_row9_col0\" class=\"data row9 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_40a0c_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_40a0c_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "      <td id=\"T_40a0c_row9_col4\" class=\"data row9 col4\" >0</td>\n",
       "      <td id=\"T_40a0c_row9_col5\" class=\"data row9 col5\" >16</td>\n",
       "      <td id=\"T_40a0c_row9_col6\" class=\"data row9 col6\" >248</td>\n",
       "      <td id=\"T_40a0c_row9_col7\" class=\"data row9 col7\" >250</td>\n",
       "      <td id=\"T_40a0c_row9_col8\" class=\"data row9 col8\" >253</td>\n",
       "      <td id=\"T_40a0c_row9_col9\" class=\"data row9 col9\" >253</td>\n",
       "      <td id=\"T_40a0c_row9_col10\" class=\"data row9 col10\" >253</td>\n",
       "      <td id=\"T_40a0c_row9_col11\" class=\"data row9 col11\" >253</td>\n",
       "      <td id=\"T_40a0c_row9_col12\" class=\"data row9 col12\" >232</td>\n",
       "      <td id=\"T_40a0c_row9_col13\" class=\"data row9 col13\" >213</td>\n",
       "      <td id=\"T_40a0c_row9_col14\" class=\"data row9 col14\" >111</td>\n",
       "      <td id=\"T_40a0c_row9_col15\" class=\"data row9 col15\" >2</td>\n",
       "      <td id=\"T_40a0c_row9_col16\" class=\"data row9 col16\" >0</td>\n",
       "      <td id=\"T_40a0c_row9_col17\" class=\"data row9 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_40a0c_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_40a0c_row10_col0\" class=\"data row10 col0\" >0</td>\n",
       "      <td id=\"T_40a0c_row10_col1\" class=\"data row10 col1\" >0</td>\n",
       "      <td id=\"T_40a0c_row10_col2\" class=\"data row10 col2\" >0</td>\n",
       "      <td id=\"T_40a0c_row10_col3\" class=\"data row10 col3\" >0</td>\n",
       "      <td id=\"T_40a0c_row10_col4\" class=\"data row10 col4\" >0</td>\n",
       "      <td id=\"T_40a0c_row10_col5\" class=\"data row10 col5\" >0</td>\n",
       "      <td id=\"T_40a0c_row10_col6\" class=\"data row10 col6\" >0</td>\n",
       "      <td id=\"T_40a0c_row10_col7\" class=\"data row10 col7\" >43</td>\n",
       "      <td id=\"T_40a0c_row10_col8\" class=\"data row10 col8\" >98</td>\n",
       "      <td id=\"T_40a0c_row10_col9\" class=\"data row10 col9\" >98</td>\n",
       "      <td id=\"T_40a0c_row10_col10\" class=\"data row10 col10\" >208</td>\n",
       "      <td id=\"T_40a0c_row10_col11\" class=\"data row10 col11\" >253</td>\n",
       "      <td id=\"T_40a0c_row10_col12\" class=\"data row10 col12\" >253</td>\n",
       "      <td id=\"T_40a0c_row10_col13\" class=\"data row10 col13\" >253</td>\n",
       "      <td id=\"T_40a0c_row10_col14\" class=\"data row10 col14\" >253</td>\n",
       "      <td id=\"T_40a0c_row10_col15\" class=\"data row10 col15\" >187</td>\n",
       "      <td id=\"T_40a0c_row10_col16\" class=\"data row10 col16\" >22</td>\n",
       "      <td id=\"T_40a0c_row10_col17\" class=\"data row10 col17\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17eb130d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "# Convert the image `im3` to a PyTorch tensor\n",
    "im3_t = tensor(im3)\n",
    "\n",
    "# Take a subregion of the tensor (rows 4 to 14, columns 4 to 21)\n",
    "# and convert it into a pandas DataFrame for easier visualization\n",
    "df = pd.DataFrame(im3_t[4:15, 4:22])\n",
    "\n",
    "# Style the DataFrame: set the font size very small and apply a grey color gradient\n",
    "# to visually represent the pixel intensity values\n",
    "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd915a-a2aa-4e54-a5d3-82de398a1064",
   "metadata": {},
   "source": [
    "Here we convert a portion of the image to a DataFrame to better visualize the pixel values.\n",
    "Darker cells correspond to higher pixel intensity, giving us a clear numeric view of the image region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6a92d-6c67-4d22-861d-948d19ac4b5e",
   "metadata": {},
   "source": [
    "So, now we have seen what an image looks like to a computer, let's recall our goal: create a model that can recognize 3s and 7s. \n",
    "\n",
    "**How might you go about getting a computer to do that?**\n",
    "\n",
    "Warning: Stop and Think!: Before you read on, take a moment to think about how a computer might be able to recognize these two different digits.\n",
    "\n",
    "*Visual differences*\n",
    "\n",
    "- What shapes or strokes make a 3 different from a 7?\n",
    "- Look at the curves and lines — which digit has a full curve at the bottom? Which has a straight diagonal line?\n",
    "  \n",
    "*Pixel-level clues*\n",
    "\n",
    "- If a computer looks at the image as a grid of pixels, which pixels might change the most between a 3 and a 7?\n",
    "- Look at edges and corners — the pixels that form the top bar of a 7, or the loops in a 3, will stand out.\n",
    "  \n",
    "*Feature representation*\n",
    "\n",
    "- How could a computer summarize these differences into simple features (like “curved vs straight”)?\n",
    "- It could compute averages of pixel intensities, detect lines or curves, or even measure distances to prototype images.\n",
    "  \n",
    "*Combining features*\n",
    "\n",
    "- Once the computer knows several features, how could it combine them to make a prediction?\n",
    "- It could assign weights to each feature and compute a score, choosing the digit whose features match best.\n",
    "\n",
    "*Generalisation*\n",
    "\n",
    "- How would the computer handle handwriting that’s messy or different from training examples?\n",
    "- The computer needs to learn patterns, not just exact pixels. That’s why neural networks learn flexible representations instead of memorizing images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017a7cf-b8ab-4931-8118-f080b9ffb58a",
   "metadata": {},
   "source": [
    "## Pixel Similarity ##\n",
    "\n",
    "How about we find the average pixel value for every pixel of the 3s, then do the same for the 7s? This will give us two group averages, defining what we might call the \"ideal\" 3 and 7. Then, to classify an image as one digit or the other, we see which of these two ideal digits the image is most similar to. This certainly seems like it should be better than nothing, so it will make a good baseline.\n",
    "\n",
    "**How Do We Do This?**\n",
    "\n",
    "Step one for our simple model is to get the average of pixel values for each of our two groups. \n",
    "\n",
    "Let's create a tensor containing all of our 3s stacked together. We already know how to create a tensor containing a single image. To create a tensor containing all the images in a directory, we will first use a Python list comprehension to create a plain list of the single image tensors.\n",
    "\n",
    "Here we load all the training images for digits 3 and 7 and convert them into tensors, preparing the data so it can be used in numerical computations and later fed into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53107d4-f422-4161-a9a8-5d2ee36ba842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open each image file for the digit 7 and convert it into a PyTorch tensor\n",
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "\n",
    "# Open each image file for the digit 3 and convert it into a PyTorch tensor\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "\n",
    "# Check how many tensor images we have for each digit\n",
    "len(three_tensors),len(seven_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b2751-6048-4b68-9159-fa98eb68c8ec",
   "metadata": {},
   "source": [
    "We'll also check that one of the images looks okay. Since we now have tensors (which Jupyter by default will print as values), rather than PIL images (which Jupyter by default will display as images), we need to use fastai's show_image function to display it.\n",
    "\n",
    "This lets us visually confirm that the tensor data has been loaded correctly by displaying one example of the digit 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91e194c4-5234-44bf-a96a-d561362f54c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the second image tensor for the digit 3 to visually inspect the data\n",
    "show_image(three_tensors[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f87b3-e7e9-4fb7-9f99-3adb717d79b7",
   "metadata": {},
   "source": [
    "For every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this we first combine all the images in this list into a single three-dimensional tensor. The most common way to describe such a tensor is to call it a rank-3 tensor. We often need to stack up individual tensors in a collection into a single tensor. Unsurprisingly, PyTorch comes with a function called stack that we can use for this purpose.\n",
    "\n",
    "In ELI5 terms:  \n",
    "\n",
    "You have a big stack of handwritten 3s on paper. Every page is:\n",
    "\n",
    "- the same size\n",
    "- made of tiny squares (pixels)\n",
    "- each square has a darkness value (how much ink is there?)\n",
    "\n",
    "Now you ask:\n",
    "\n",
    "“If I look at the top-left square across all the pages, how dark is it on average?”\n",
    "\n",
    "Then you ask the same question for:\n",
    "\n",
    "- the next square\n",
    "- and the next\n",
    "- and the next…\n",
    "\n",
    "Until you’ve done it for every pixel position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a3abda-3a5c-4268-ae7a-067f091f6664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack all the 7 image tensors into a single 3D tensor\n",
    "# Shape: (number_of_images, height, width)\n",
    "# Convert values to floats and scale pixel intensities from 0–255 to 0–1\n",
    "stacked_sevens = torch.stack(seven_tensors).float() / 255\n",
    "\n",
    "# Do the same for all the 3 image tensors\n",
    "stacked_threes = torch.stack(three_tensors).float() / 255\n",
    "\n",
    "# Check the shape of the stacked tensor to confirm how the data is organized\n",
    "stacked_threes.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e1303-8dde-4729-b2de-d2c5358a69a3",
   "metadata": {},
   "source": [
    "### Tensors ###\n",
    "\n",
    "Rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor.  The ELI5 version is to think of it like this:\n",
    "\n",
    "**Imagine bookshelves 📚**\n",
    "\n",
    "📖 Rank = how many “levels” of organization\n",
    "\n",
    "- Rank 0 → a single book\n",
    "- Rank 1 → a row of books on one shelf\n",
    "- Rank 2 → a bookshelf with multiple shelves\n",
    "- Rank 3 → a room full of bookshelves\n",
    "\n",
    "We can see the rank of our tensor by looking at the length of a tensor's shape - its rank.\n",
    "\n",
    "This tells us how many axes (dimensions) the tensor has.  For stacked MNIST images, this should be 3: number of images, height, and width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86043aee-2374-4c00-9e20-c3bcbb601d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of dimensions (rank) of the stacked_threes tensor\n",
    "len(stacked_threes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3170cf-6b86-45cd-93f3-f81734809827",
   "metadata": {},
   "source": [
    "📐 Shape = how many items at each level\n",
    "\n",
    "shape = (5, 4, 20)\n",
    "\n",
    "Means:\n",
    "\n",
    "- 5 bookshelves (rooms don’t matter here)\n",
    "- each shelf has 4 shelves\n",
    "- each shelf holds 20 books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc61ae6b-26cc-406b-b4af-b193b4d82adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd16f5b-71c6-4a86-a5eb-58c56a449417",
   "metadata": {},
   "source": [
    "So the shape of our tensor is:\n",
    "\n",
    "**6131**\n",
    "You have 6,131 images\n",
    "Each image is one handwritten 3\n",
    "\n",
    "**28**\n",
    "Each image is 28 pixels tall\n",
    "\n",
    "**28**\n",
    "Each image is 28 pixels wide\n",
    "\n",
    "So altogether this tensor is a stack of 6,131 images, where each image is 28×28 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85080595-d508-4304-9488-6e301070c32f",
   "metadata": {},
   "source": [
    "Finally, we can compute what the ideal 3 looks like. We calculate the mean of all the image tensors by taking the mean along dimension 0 of our stacked, rank-3 tensor. This is the dimension that indexes over all the images.\n",
    "\n",
    "In other words, for every pixel position, this will compute the average of that pixel over all images. The result will be one value for every pixel position, or a single image. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e50988a6-cf6e-4748-ba8c-252156062744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the average pixel intensity at each pixel position\n",
    "# across all images of the digit 3\n",
    "mean3 = stacked_threes.mean(0)\n",
    "\n",
    "# Display the resulting \"average 3\" image\n",
    "show_image(mean3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1ba16-d21c-4fc0-91d0-238c0b153117",
   "metadata": {},
   "source": [
    "**We are basically creating a prototype digit!**\n",
    "\n",
    "Let's do the same thing for the 7s, but put all the steps together at once to save some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e06390-e98e-4dcb-ba23-0af88b1e0ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the average pixel intensity at each pixel position\n",
    "# across all images of the digit 7\n",
    "mean7 = stacked_sevens.mean(0)\n",
    "\n",
    "# Display the resulting \"average 7\" image\n",
    "show_image(mean7);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2f114-76ab-4e02-b76b-3470d0eaaf52",
   "metadata": {},
   "source": [
    "Let's now pick an arbitrary 3 and measure its distance from our \"ideal digits.\"\n",
    "\n",
    "**How could we do this?**\n",
    "\n",
    "Think of:\n",
    "\n",
    "- putting two pictures on top of each other\n",
    "- checking how different each tiny square (pixel) is\n",
    "- then adding up all those differences\n",
    "  \n",
    "If:\n",
    "\n",
    "- the picture is very similar → distance is small\n",
    "- the picture is very different → distance is large\n",
    "\n",
    "So the smaller distance = more similar.\n",
    "\n",
    "Let's pick a sample 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e98fd4a5-f2f7-47cf-8ca3-b1d6a5d71c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the second image from the stacked_threes tensor\n",
    "a_3 = stacked_threes[1]\n",
    "\n",
    "# Display this specific handwritten 3 image\n",
    "show_image(a_3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb99625-c802-4d95-9263-56cc17ef677f",
   "metadata": {},
   "source": [
    "**How can we determine its distance from our ideal 3?**\n",
    "\n",
    "Here are two main ways data scientists measure distance in this context:\n",
    "\n",
    "1. Take the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm.  **ELI5:  The mean absolute difference is like checking every little thermometer (pixel), seeing how far apart the readings are without worrying about sign, and then averaging all those differences to get a single “how different” number.**\n",
    "     \n",
    "     \n",
    "\n",
    "2.  Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm. **ELI5:  Each pixel is like a checkpoint**\n",
    "\n",
    "   - Compare two images pixel by pixel:\n",
    "   - Subtract pixels\n",
    "   - Square the differences → makes everything positive and exaggerates big differences\n",
    "   - Average over all pixels\n",
    "   - Take square root → get RMSE\n",
    "   - Small RMSE → images are similar\n",
    "   - Large RMSE → images are very different\n",
    "\n",
    "Let's try both of these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16bb1d6f-46ee-492d-a3df-5af039576840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1114), tensor(0.2021))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the mean absolute difference (L1 distance) between the selected 3\n",
    "# and the average (ideal) 3 image\n",
    "dist_3_abs = (a_3 - mean3).abs().mean()\n",
    "\n",
    "# Compute the root mean squared error (L2 distance) between the selected 3\n",
    "# and the average (ideal) 3 image\n",
    "dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n",
    "\n",
    "# Display both distance values for comparison\n",
    "dist_3_abs, dist_3_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9da205-f527-4b06-a52b-1e79a39672c0",
   "metadata": {},
   "source": [
    "**So what does this tell us?**\n",
    "\n",
    "0.1114 → L1 distance (mean absolute difference)\n",
    "\n",
    "- On average, each pixel differs by about 0.11\n",
    "- Since pixel values are between 0 and 1, this is fairly small\n",
    "- Meaning: the image looks quite similar to the average 3\n",
    "\n",
    "0.2021 → L2 distance (RMSE)\n",
    "\n",
    "- This version penalizes big differences more\n",
    "- It’s a bit larger because squaring emphasizes pixels that differ a lot\n",
    "- Still: not huge → the image is reasonably close to the ideal 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "927c6978-e055-4579-92d2-bf46f9416c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the mean absolute difference (L1 distance) between the selected 3 and the average (ideal) 7 image\n",
    "dist_7_abs = (a_3 - mean7).abs().mean()\n",
    "\n",
    "# Compute the root mean squared error (L2 distance) between the selected 3 and the average (ideal) 7 image\n",
    "dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n",
    "\n",
    "# Display both distance values for comparison\n",
    "dist_7_abs, dist_7_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43232778-391e-4941-924a-b40037658393",
   "metadata": {},
   "source": [
    "In both cases, the distance between our 3 and the \"ideal\" 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case.\n",
    "\n",
    "PyTorch already provides both of these as loss functions. You'll find these inside torch.nn.functional, which the PyTorch team recommends importing as F (and is available by default under that name in fastai).  These give the same results as our manual calculations, but are more concise and commonly used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "763ba1f3-3774-45c1-acd0-c2017a911510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute distances using PyTorch's built-in loss functions:\n",
    "# - L1 loss (mean absolute difference)\n",
    "# - Mean squared error (MSE), followed by square root to get RMSE (L2 distance)\n",
    "F.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f8437-9775-4629-94d9-10e050795aad",
   "metadata": {},
   "source": [
    "## NumPy Arrays & PyTorch Tensors ##\n",
    "\n",
    "NumPy is the most widely used library for scientific and numeric programming in Python. It provides very similar functionality and a very similar API to that provided by PyTorch; however, **it does not support using the GPU or calculating gradients*, which are both critical for deep learning. Therefore, we will generally use PyTorch tensors instead of NumPy arrays, where possible.\n",
    "\n",
    "**But what are arrays and tensors, and why should you care?**\n",
    "\n",
    "Python is slow compared to many languages. Anything fast in Python, NumPy, or PyTorch is likely to be a wrapper for a compiled object written (and optimized) in another language—specifically C. In fact, NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure Python.\n",
    "\n",
    "**NumPy array = very flexible table**\n",
    "\n",
    "A NumPy array is like a table (or a stack of tables) where:\n",
    "\n",
    "- Everything is in rows and columns (or more dimensions)\n",
    "- All items are the same type (like all numbers)\n",
    "\n",
    "But NumPy is flexible:\n",
    "\n",
    "- You can even put tables inside tables\n",
    "- Some inner tables can be different sizes\n",
    "\n",
    "That’s called a jagged array.\n",
    "\n",
    "*Example idea:*\n",
    "\n",
    "- Row 1 has 3 numbers\n",
    "- Row 2 has 5 numbers\n",
    "- Row 3 has 2 numbers\n",
    "\n",
    "NumPy allows that (if you’re careful).  When NumPy stores simple numbers (ints or floats), it packs them very tightly in memory, like a super-organized filing cabinet.  That’s why NumPy is fast — it uses highly optimized C code behind the scenes.\n",
    "\n",
    "**PyTorch tensor = strict, neat table**\n",
    "\n",
    "A PyTorch tensor is almost the same idea, but stricter:\n",
    "\n",
    "- It must be a perfect rectangle\n",
    "- Every row has the same length\n",
    "- Every layer has the same shape\n",
    "- All values must be the same basic numeric type\n",
    "- No jagged edges allowed \n",
    "\n",
    "Think:\n",
    "✔️ 28×28 images stacked neatly\n",
    "❌ rows with different lengths\n",
    "\n",
    "Because everything is so regular:\n",
    "\n",
    "- PyTorch can do automatic differentiation\n",
    "- PyTorch can run on GPUs\n",
    "- Math becomes easier and faster for neural networks\n",
    "\n",
    "The vast majority of methods and operators supported by NumPy on these structures are also supported by PyTorch, but PyTorch tensors have additional capabilities. One major capability is that these structures can live on the GPU, in which case their computation will be optimized for the GPU and can run much faster (given lots of values to work on). In addition, PyTorch can automatically calculate derivatives of these operations, including combinations of operations. As you'll see, it would be impossible to do deep learning in practice without this capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a608f-4f71-483b-9f03-ed87270bc636",
   "metadata": {},
   "source": [
    "### How to use Array/Tensor APIs\n",
    "\n",
    "Perhaps the most important new coding skill for a Python programmer to learn is how to effectively use the array/tensor APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9cd0a-3952-45b3-8b72-2483fdafde1c",
   "metadata": {},
   "source": [
    "### Creating & Displaying Arrays / Tensors ###\n",
    "\n",
    "To create an array or tensor, pass a list (or list of lists, or list of lists of lists, etc.) to array() or tensor():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "666a945b-d02d-444b-81fd-3aca33488de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python list of lists (2D structure)\n",
    "data = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "# Convert the Python list into a NumPy array\n",
    "arr = array(data)\n",
    "\n",
    "# Convert the Python list into a tensor (e.g. PyTorch tensor)\n",
    "tns = tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd75a0a3-4ea1-48ee-9c2d-05325b69097b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr  # NumPy array: displays the contents of the array in a notebook cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0614b47f-985c-4d5a-9079-d6f3d68cb74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns  # PyTorch tensor: displays the tensor values in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0e4bb-d945-4086-ac31-d1e19f80aebd",
   "metadata": {},
   "source": [
    "All the operations that follow are shown on tensors, but the syntax and results for NumPy arrays is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ce746e4-c5a1-4256-a5ec-f0d6e014b302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[1]  # Selects the second row of the tensor (indexing starts at 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa738f-d938-4839-b10a-ff0c547fdde2",
   "metadata": {},
   "source": [
    "Or a column, by using : to indicate all of the first axis (we sometimes refer to the dimensions of tensors/arrays as axes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33cc8565-bdea-46a8-b865-276316a09aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[:, 1]  # Selects the second column of the tensor (all rows, column index 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e869c749-7364-4989-8238-be678d3bd0ba",
   "metadata": {},
   "source": [
    "You can combine these with Python slice syntax ([start:end] with end being excluded) to select part of a row or column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a293361a-c6f2-45a6-b752-db44a75e5f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[1, 1:3]  # Returns tensor([5, 6]) — slice of the second row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015cbbd9-eb2a-433d-b218-1d5ce1eacf3c",
   "metadata": {},
   "source": [
    "And you can use the standard operators such as +, -, *, /:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2873cc85-d146-4461-acaf-2aa6dd770e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns + 1  # Adds 1 to every element in the tensor (element-wise operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549002be-07bc-4cc2-bd8f-fc8d3f1f8c18",
   "metadata": {},
   "source": [
    "Tensors have a type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd631f80-d300-472d-ba6a-ce720bc82824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51955359-e3b6-4594-9dc5-82d809828dbd",
   "metadata": {},
   "source": [
    "And will automatically change type as needed, for example from int to float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49f54dab-6b01-4fa7-b8fe-ef7b342457c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5000, 3.0000, 4.5000],\n",
       "        [6.0000, 7.5000, 9.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns * 1.5  # Multiplies every element in the tensor by 1.5 (element-wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb40519-a896-4e69-ba0f-94579f8a46a7",
   "metadata": {},
   "source": [
    "So, is our baseline model any good? To quantify this, we must define a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9cc9c-5c76-44f3-b2cc-fbfb23ba0b8c",
   "metadata": {},
   "source": [
    "## Computing Metrics Using Broadcasting ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b532c-c0a9-40e5-a01b-b7a1d7522291",
   "metadata": {},
   "source": [
    "A metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is. For instance, we could use either of the functions we saw in the previous section, mean squared error, or mean absolute error, and take the average of them over the whole dataset. However, neither of these are numbers that are very understandable to most people; in practice, we normally use accuracy as the metric for classification models.\n",
    "\n",
    "We want to calculate our metric over a validation set. This is so that we don't inadvertently overfit—that is, train a model to work well only on our training data. This is not really a risk with the pixel similarity model we're using here as a first try, since it has no trained components, but we'll use a validation set anyway to follow normal practices and to be ready for our second try later.\n",
    "\n",
    "To get a validation set we need to remove some of the data from training entirely, so it is not seen by the model at all. As it turns out, the creators of the MNIST dataset have already done this for us. There was a whole separate directory called valid. That's what this directory is for!\n",
    "\n",
    "So to start with, let's create tensors for our 3s and 7s from that directory. These are the tensors we will use to calculate a metric measuring the quality of our first-try model, which measures distance from an ideal image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9594d4a5-a559-4ce3-a9ce-5200b8c548d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all images of the digit '3' from the validation folder, convert them to tensors, \n",
    "# and stack them into a single 4D tensor (number of images, height, width, channels)\n",
    "valid_3_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'valid'/'3').ls()])\n",
    "\n",
    "# Convert the stacked tensor to float and normalize pixel values to range [0,1]\n",
    "valid_3_tens = valid_3_tens.float()/255\n",
    "\n",
    "# Load all images of the digit '7' from the validation folder, convert them to tensors, \n",
    "# and stack them into a single 4D tensor\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'valid'/'7').ls()])\n",
    "\n",
    "# Convert the stacked tensor to float and normalize pixel values to range [0,1]\n",
    "valid_7_tens = valid_7_tens.float()/255\n",
    "\n",
    "# Check the shapes of the resulting tensors for '3' and '7'\n",
    "valid_3_tens.shape, valid_7_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930baf6-31a0-42bc-8de3-87d7be3c909b",
   "metadata": {},
   "source": [
    "- This code loads all images of digits 3 and 7 from the validation dataset.\n",
    "- Converts them into PyTorch tensors.\n",
    "- Normalises pixel values to the [0,1] range.\n",
    "- Returns the shape of each tensor, which will be (number_of_images, height, width) if grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938a4e5-efde-4db0-b089-ed5543c498ec",
   "metadata": {},
   "source": [
    "Here we see two tensors, one representing the 3s validation set of 1,010 images of size 28×28, and one representing the 7s validation set of 1,028 images of size 28×28.\n",
    "\n",
    "We ultimately want to write a function, is_3, that will decide if an arbitrary image is a 3 or a 7. It will do this by deciding which of our two \"ideal digits\" this arbitrary image is closer to. For that we need to define a notion of distance—that is, a function that calculates the distance between two images.\n",
    "\n",
    "We can write a simple function that calculates the mean absolute error using an expression very similar to the one we wrote in the last section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dba46e66-0186-44f2-ab08-8b94a63834d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1114)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to compute the \"distance\" between two MNIST images/tensors\n",
    "# a and b are tensors representing images (or batches of images)\n",
    "# The function computes the mean absolute difference per pixel across the last two dimensions (height and width)\n",
    "def mnist_distance(a, b): \n",
    "    return (a - b).abs().mean((-1, -2))\n",
    "\n",
    "# Compute the distance between a specific image a_3 and the mean image of all 3s (mean3)\n",
    "mnist_distance(a_3, mean3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc8248-1011-41d4-a2dc-cf7642e06f12",
   "metadata": {},
   "source": [
    "This function measures how similar two MNIST images are, with smaller values meaning more similar:\n",
    "\n",
    "- (a - b) → subtracts the pixel values of b from a.\n",
    "- .abs() → takes the absolute value of the difference (so negative differences don’t cancel out positives).\n",
    "- .mean((-1,-2)) → averages over the last two dimensions, which are the height and width of the image, giving a single number representing the \"average pixel difference.\"\n",
    "\n",
    "tensor(0.1114)\n",
    "\n",
    "This is the same value we previously calculated for the distance between these two images, the ideal 3 mean3 and the arbitrary sample 3 a_3, which are both single-image tensors with a shape of [28,28].\n",
    "\n",
    "But in order to calculate a metric for overall accuracy, we will need to calculate the distance to the ideal 3 for every image in the validation set. How do we do that calculation?\n",
    "\n",
    "Something very interesting happens when we take this exact same distance function, designed for comparing two single images, but pass in as an argument valid_3_tens, the tensor that represents the 3s validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "078f4407-579a-4d7b-b952-e3139169b0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1634, 0.1145, 0.1363,  ..., 0.1105, 0.1111, 0.1640]),\n",
       " torch.Size([1010]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the distance between each image in the validation set of 3s (valid_3_tens)\n",
    "# and the mean image of all 3s (mean3) using the mnist_distance function.\n",
    "valid_3_dist = mnist_distance(valid_3_tens, mean3)\n",
    "\n",
    "# Display the resulting distances and their shape.\n",
    "# valid_3_dist is a 1D tensor of length equal to the number of images (here, 1010),\n",
    "# where each entry is the average pixel difference between an image and mean3.\n",
    "valid_3_dist, valid_3_dist.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468218d4-120f-4f9a-82b2-4a43492c7843",
   "metadata": {},
   "source": [
    "PyTorch uses broadcasting when shapes don’t exactly match.\n",
    "valid_3_tens has shape (1010, 28, 28)\n",
    "mean3 has shape (28, 28)\n",
    "When you do valid_3_tens - mean3, PyTorch automatically “stretches” mean3 across the batch dimension: it acts like you subtracted mean3 from each of the 1010 images individually.\n",
    "Think of it like copying mean3 1010 times “behind the scenes” without actually using more memory. \n",
    "\n",
    "But what does that mean in plain English?  Lets use a simple analogy:\n",
    "\n",
    "- Imagine 1010 pancakes stacked.\n",
    "- You have one special pancake (mean3).\n",
    "- You lay that pancake on top of each pancake in the stack.\n",
    "- For each pancake, measure how different it is from the top pancake.\n",
    "- You get a list of 1010 difference scores.\n",
    "\n",
    "mean3 isn’t just a random pancake. It’s the average of all the 3s in the training set.  Comparing each validation image to this average lets us measure how typical or unusual each 3 is.  Using a random 3 would work, but it would be biased—your “distance” would depend too much on the quirks of that one image. Using the average makes it stable and representative.\n",
    "\n",
    "The magic trick is that PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use broadcasting. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write.  PyTorch basically knows to loop over 1010 images in the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52459ae-65f7-424b-bf23-27974cca3bc3",
   "metadata": {},
   "source": [
    "**Broadcasting**\n",
    "\n",
    "Lets keep this simple...\n",
    "\n",
    "We have two tensors:\n",
    "\n",
    "tensor([1,2,3])     → shape (3)\n",
    "\n",
    "tensor(1)           → shape ()\n",
    "\n",
    "One is a list of 3 numbers.\n",
    "One is just a single number.\n",
    "\n",
    "PyTorch says:\n",
    "“Oh, you want to add 1 to every number in the list.”\n",
    "\n",
    "So it pretends the 1 is actually:\n",
    "tensor([1,1,1])\n",
    "\n",
    "Now both tensors are shape (3). That pretending step is broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "155b2bf9-d398-492b-8bfc-c4e96b775f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor([1,2,3]) + tensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c8fe7-858d-4c79-a342-8c57e3bab736",
   "metadata": {},
   "source": [
    "We are calculating the difference between our \"ideal 3\" and each of the 1,010 3s in the validation set, for each of 28×28 images, resulting in the shape [1010,28,28].\n",
    "\n",
    "There are a couple of important points about how broadcasting is implemented, which make it valuable not just for expressivity but also for performance:\n",
    "\n",
    "PyTorch doesn't actually copy mean3 1,010 times. It pretends it were a tensor of that shape, but doesn't actually allocate any additional memory\n",
    "It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!).\n",
    "This is true of all broadcasting and elementwise operations and functions done in PyTorch. It's the most important technique for you to know to create efficient PyTorch code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2cf7b4-ec01-4390-8f50-822eacb12494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAI Env",
   "language": "python",
   "name": "fastai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
